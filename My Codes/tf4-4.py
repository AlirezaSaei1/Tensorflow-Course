import csv
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

# Function for windowing the data
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    dataset = tf.data.Dataset.from_tensor_slices(series)
    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda w: w.batch(window_size + 1))
    dataset = dataset.shuffle(shuffle_buffer).map(lambda w: (w[:-1], w[-1]))
    dataset = dataset.batch(batch_size).prefetch(1)
    return dataset

# Lists for dataset
time_step = []
sunspots = []

# Read dataset
with open('TimeSeries/Sunspots.csv') as f:
    reader = csv.reader(f, delimiter=',')
    next(reader)
    for row in reader:
        sunspots.append(float(row[2]))
        time_step.append(int(row[0]))


# Convert lists to numpy arrays - everytime we append sth to numpy array lots of memory management for cloning the list
# so it is better to first create a throwaway list then convert it to numpy arrays
series = np.array(sunspots)
time = np.array(time_step)


# Plot the data
plt.plot(time, series)
plt.show()


# Data split
split_time = 3000  # Total data: 3,500
time_train = time[:split_time]
x_train = series[:split_time]
time_valid = time[split_time:]
x_valid = series[split_time:]


# Variables
window_size = 30 # Can change after encountering bad results 20, 30, ...
batch_size = 32
shuffle_buffer_size = 1000


# Windowed dataset
dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)


# Reset states generated by Keras
tf.keras.backend.clear_session()

# Build the Model
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=64, kernel_size=3,
                      strides=1,
                      activation="relu",
                      padding='causal',
                      input_shape=[window_size, 1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400)
])

 # Print the model summary 
model.summary()

# Set the learning rate
learning_rate = 2e-5

# Set the optimizer 
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)

# Set the training parameters
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

# Train the model
history = model.fit(x_train ,epochs=100)

